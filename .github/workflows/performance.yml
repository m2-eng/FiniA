name: Performance Tests

on:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    # Allow manual trigger
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'tests/performance/**'

jobs:
  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: test_root_password
          MYSQL_DATABASE: finia_test
          MYSQL_USER: finia_test
          MYSQL_PASSWORD: test_password
        ports:
          - 3306:3306
        options: >-
          --health-cmd="mysqladmin ping --silent"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark pytest-html
      
      - name: Wait for MySQL
        run: |
          for i in {1..30}; do
            if mysqladmin ping -h 127.0.0.1 -P 3306 -u root -ptest_root_password --silent; then
              echo "MySQL is ready"
              break
            fi
            echo "Waiting for MySQL... ($i/30)"
            sleep 2
          done
      
      - name: Create test database schema
        run: |
          mysql -h 127.0.0.1 -P 3306 -u root -ptest_root_password finia_test < db/migrations/001_initial_schema.sql
        continue-on-error: true
      
      - name: Create .env.test file
        run: |
          cat > .env.test << EOF
          DB_TEST_USER=finia_test
          DB_TEST_PASSWORD=test_password
          DB_TEST_HOST=127.0.0.1
          DB_TEST_PORT=3306
          DB_TEST_NAME=finia_test
          TEST_API_BASE_URL=http://127.0.0.1:8000
          TEST_API_TIMEOUT=30
          FINIA_TEST_USER=local_test
          CRITICAL_TIMEOUT_MS=2000
          WARNING_TIMEOUT_MS=1000
          EOF
      
      - name: Start FiniA API Server
        run: |
          python src/main.py &
          sleep 10
          echo "API_PID=$!" >> $GITHUB_ENV
        env:
          DB_USER: finia_test
          DB_PASSWORD: test_password
          DB_HOST: 127.0.0.1
          DB_PORT: 3306
          DB_NAME: finia_test
      
      - name: Run Performance Benchmarks
        run: |
          pytest tests/performance/ \
            -m performance \
            --benchmark-only \
            --benchmark-json=reports/benchmark.json \
            --html=reports/performance_report.html \
            --self-contained-html \
            -v
      
      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: reports/benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          fail-on-alert: false
          alert-threshold: '150%'
      
      - name: Upload Performance Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-reports
          path: reports/
          retention-days: 90
      
      - name: Stop API Server
        if: always()
        run: |
          if [ ! -z "$API_PID" ]; then
            kill $API_PID || true
          fi
      
      - name: Check Performance Regression
        run: |
          # Check if performance degraded significantly
          python -c "
          import json
          with open('reports/benchmark.json', 'r') as f:
              data = json.load(f)
          
          # Check year overview endpoint (Issue #66 fix validation)
          for benchmark in data.get('benchmarks', []):
              if 'year_overview' in benchmark.get('name', '').lower():
                  mean_time = benchmark['stats']['mean']
                  if mean_time > 2.0:  # 2 seconds critical threshold
                      print(f'❌ PERFORMANCE REGRESSION: {benchmark[\"name\"]} took {mean_time:.2f}s (> 2s)')
                      exit(1)
          
          print('✅ Performance tests passed')
          "
